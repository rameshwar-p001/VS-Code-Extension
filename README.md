# Chat Assistant Extension

This is a VS Code sidebar extension that allows you to chat with a locally running LLM (DeepSeek Coder) using Ollama. It supports prompt sending, response display, and a nice chat-style interface.

## Features
- Send prompts to DeepSeek Coder via localhost
- Receive and display responses in a webview
- Chat-style interface
# ğŸ’¬ VS Code Chat Assistant (Local LLM Integration)

This is a powerful VS Code extension that brings an AI chat assistant directly into your sidebar â€” **no internet, no API key required**. It connects to a locally running Large Language Model (LLM) like **DeepSeek Coder via Ollama**, and helps you with:

- ğŸ’» Code suggestions  
- â“ Error explanations  
- ğŸ“˜ Learning programming concepts  
- ğŸ” Chat-style history with context

---

## âš™ï¸ Features

- ğŸ§  Connects to local LLM (DeepSeek Coder)
- ğŸª„ Clean, minimal, and modern sidebar UI
- ğŸ’¬ Chat history in a scrollable panel
- ğŸ” Loop support for continuous interaction
- ğŸ“¤ Coming Soon: Save, Export, and Clear buttons!

---

## ğŸš€ Setup Instructions

1. Install [Ollama](https://ollama.com/) and run:
   ```bash
   ollama run deepseek-coder
Clone this repo and open it in VS Code.

Run the extension:

Press F5 to open a new Extension Development Host window.

Open the Command Palette (Ctrl+Shift+P) â†’ Chat Assistant: Start.

ğŸ“ Tech Stack
ğŸ§© JavaScript (VS Code Extension API)

ğŸ¨ HTML + CSS (for custom UI)

ğŸ”— HTTP (connects to localhost:11434 for model inference)

ğŸ§  DeepSeek Coder via Ollama

ğŸ™Œ Credits
Made with â¤ï¸ by Patil Rameshwar D
ğŸ“ Pimpri Chinchwad University
Special thanks to Mrs. Ruchira K Karanjikar for constant guidance.

ğŸ“Œ Screenshot

ğŸ“Œ Note
This is an experimental personal project integrating LLMs locally. Feel free to fork and improve it!
