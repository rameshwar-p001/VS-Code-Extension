# Chat Assistant Extension

This is a VS Code sidebar extension that allows you to chat with a locally running LLM using Ollama. It supports prompt sending, response display, and a nice chat-style interface.

## Features
- Send prompts to DeepSeek Coder via localhost
- Receive and display responses in a webview
- Chat-style interface
# 💬 VS Code Chat Assistant (Local LLM Integration)

This is a powerful VS Code extension that brings an AI chat assistant directly into your sidebar — **no internet, no API key required**. It connects to a locally running Large Language Model (LLM) like **DeepSeek Coder via Ollama**, and helps you with:

- 💻 Code suggestions  
- ❓ Error explanations  
- 📘 Learning programming concepts  
- 🔁 Chat-style history with context

---

## ⚙️ Features

- 🧠 Connects to local LLM (DeepSeek Coder)
- 🪄 Clean, minimal, and modern sidebar UI
- 💬 Chat history in a scrollable panel
- 🔁 Loop support for continuous interaction
- 📤 Coming Soon: Save, Export, and Clear buttons!

---

## 🚀 Setup Instructions

1. Install [Ollama](https://ollama.com/) and run:
   ```bash
   ollama run tinyLlama
2. Clone this repo and open it in VS Code.

3. Run the extension:

      Press F5 to open a new Extension Development Host window.

      Open the Command Palette (Ctrl+Shift+P) → Chat Assistant: Start.

📁 Tech Stack
🧩 JavaScript (VS Code Extension API)

🎨 HTML + CSS (for custom UI)

🔗 HTTP (connects to localhost:11434 for model inference)

🧠 TinyLlma Coder via Ollama

🙌 Credits
Made with ❤️ by Patil Rameshwar D


📌 Screenshot

![WhatsApp Image 2025-04-09 at 20 19 52_7e96fcf8](https://github.com/user-attachments/assets/eb6594e6-9a8e-4d9e-b569-50ff9bdb1d85)


![image](https://github.com/user-attachments/assets/60091937-11e1-45ff-9867-582e1e4ed44c)



📌 Note
This is an experimental personal project integrating LLMs locally. Feel free to fork and improve it!
